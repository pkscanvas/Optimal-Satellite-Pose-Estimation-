{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Speedv2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkscanvas/Optimal-Satellite-Pose-Estimation-/blob/master/Copy_of_Copy_of_Speedv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijyzTZILtffM",
        "colab_type": "text"
      },
      "source": [
        "**Mounting the Google Drive to access the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KGc77qXZdvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT5LcSHct93p",
        "colab_type": "text"
      },
      "source": [
        "Importing all necessary packages.\n",
        "\n",
        "\n",
        "*   The **preprocess_input** function is meant to adequate your image to the format the model requires\n",
        "*  **ImageDataGenerator:** Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXvaq5cyeZbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dataset_root=\"/content/gdrive/My Drive/Colab Notebooks/speed\"\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import Xception\n",
        "from keras.applications.xception import preprocess_input  \n",
        "from keras.models import Model\n",
        "from keras.layers import Dense,Flatten,BatchNormalization,Activation,AlphaDropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErLq9d5uw1po",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   **with:** there is no need to call file.close() when using with statement. The with statement itself ensures proper acquisition and release of resource, it also takes care of all the exceptions by itself\n",
        "2.   **os.path** contains functions for manipulating filenames and directory names.\n",
        "3.**os.path.join()** function will add an extra slash to the pathname before joining it to the filename.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQTpWUJ8fkim",
        "colab_type": "code",
        "outputId": "4cc64d7c-dad9-4220-f167-46cac9424d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open(os.path.join(dataset_root,'train.json'),'r') as train:\n",
        "  train_json=json.load(train)\n",
        "train_json[0]['q_vbs2tango']\n",
        "#len(train_json)\n",
        "type(train_json)\n",
        "# q=train_json[i]['q_vbs2tango']\n",
        "# r=train_json[i]['r_Vo2To_vbs_true']\n",
        "# a=q.extend(r)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y12IBNK5jVlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgname_list=[]\n",
        "for i in range(len(train_json)):\n",
        "  imgname_list.append(train_json[i]['filename'])\n",
        "\n",
        "#imgname_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDL69KMEzqD_",
        "colab_type": "text"
      },
      "source": [
        "Putting all the images and corresponding 7 continuous values (4 from quaternion vector, 3 from position vector) in a Pandas dataframe to be able to use with **flow_from_dataframe** function\n",
        "for multilabel regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLbgD2bUlzzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_list=[]\n",
        "q1,q2,q3,q4,r1,r2,r3=[],[],[],[],[],[],[]\n",
        "for i in range(len(train_json)):\n",
        "  q1.append(train_json[i]['q_vbs2tango'][0])\n",
        "  q2.append(train_json[i]['q_vbs2tango'][1])\n",
        "  q3.append(train_json[i]['q_vbs2tango'][2])\n",
        "  q4.append(train_json[i]['q_vbs2tango'][3])\n",
        "  r1.append(train_json[i]['r_Vo2To_vbs_true'][0])\n",
        "  r2.append(train_json[i]['r_Vo2To_vbs_true'][1])\n",
        "  r3.append(train_json[i]['r_Vo2To_vbs_true'][2])\n",
        "  #q.extend(r)\n",
        "  #label_list.append(q)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sfy7lfwlMxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_dict={'image_names':imgname_list,'q1':q1,'q2':q2,'q3':q3,'q4':q4,'r1':r1,'r2':r2,'r3':r3}\n",
        "df=pd.DataFrame(col_dict)\n",
        "#df['labels']=label_list\n",
        "df_train,df_test=train_test_split(df,test_size=0.2)\n",
        "len(df_test)\n",
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GqXxArr5uxZ",
        "colab_type": "text"
      },
      "source": [
        "Previously, one should have to write a custom generator if they have to perform regression or predict multiple columns and utilize the image augmentation capabilities of the ImageDataGenerator, now we can have the target values as just another column/s (must be numerical datatype) in our dataframe, simply provide the column names to the **flow_from_dataframe** and we can now use all the augmentations provided by the ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcFFc9ZwsKjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=\"/content/gdrive/My Drive/Colab Notebooks/speed/images/train\"\n",
        "cols=['q1','q2','q3','q4','r1','r2','r3']\n",
        "datagen=ImageDataGenerator(preprocessing_function=preprocess_input,zoom_range=0.2,brightness_range=[0.8,1.2]) #Values less than 1.0 darken the image, e.g. [0.5, 1.0], whereas values larger than 1.0 brighten the image, e.g. [1.0, 1.5], where 1.0 has no effect on brightness.\n",
        "datagen2=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "train_generator=datagen.flow_from_dataframe(df_train,directory=path,x_col='image_names',y_col=cols,class_mode='other',target_size=(299,299),batch_size=32,color_mode='rgb')\n",
        "validation_generator=datagen2.flow_from_dataframe(df_test,directory=path,x_col='image_names',y_col=cols,class_mode='other',target_size=(299,299),batch_size=32,color_mode='rgb')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz7seWTinME_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U3TITUYL-GR",
        "colab_type": "text"
      },
      "source": [
        "**Building our model using Transfer Learning.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1corPUq8Aq8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#tensorflow.keras.backend.set_learning_phase(0)\n",
        "model_pretrained=Xception(weights='imagenet',include_top=False,input_shape=(299,299,3))\n",
        "#tensorflow.keras.backend.set_learning_phase(1)\n",
        "\n",
        "#using keras functional api to build our model\n",
        "\n",
        "x=model_pretrained.output\n",
        "\n",
        "#x=Flatten()(x)\n",
        "\n",
        "#x=Dense(32,activation='selu',kernel_initializer='lecun_normal')(x)\n",
        "        \n",
        "#x=BatchNormalization()(x)\n",
        "\n",
        "x=Dense(512,activation='elu',kernel_initializer='he_normal')(x) #\n",
        "   \n",
        "#x=AlphaDropout(0.1)(x)\n",
        "        \n",
        "# x=Dense(64,activation='selu',kernel_initializer='lecun_normal')(x) #\n",
        "   \n",
        "# x=AlphaDropout(0.1)(x)\n",
        "\n",
        "#x=BatchNormalization()(x)\n",
        "        \n",
        "#x=Dense(128,activation='selu',kernel_initializer='lecun_normal')(x)\n",
        "        \n",
        "x=Flatten()(x)\n",
        "        \n",
        "x=Dense(7,activation='linear')(x)\n",
        "\n",
        "model=Model(inputs=model_pretrained.input,outputs=x)\n",
        "\n",
        "\n",
        "# step_size_train=train_generator.n//train_generator.batch_size #It should typically be equal to the number of samples of your dataset divided by the batch size.\n",
        "# step_size_valid=validation_generator.n//validation_generator.batch_size\n",
        "# history=model.fit_generator(train_generator,steps_per_epoch=step_size_train,epochs=2,validation_data=validation_generator,validation_steps=step_size_valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4s9o3uatNyf",
        "colab_type": "text"
      },
      "source": [
        "**IMP** Which layers to train or not should be done before model compilation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3ibRWiTdunz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_generator[0][0][0][0][0]\n",
        "# train_generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuJE7xvu-o-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,j in enumerate(model.layers):\n",
        "  print(i,':',j)\n",
        "for layer in model.layers[:17]:\n",
        "  layer.trainable=True\n",
        "for layer in model.layers[17:]:\n",
        "  layer.trainable=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOnecUjTQuCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ52TuABAa8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sgd=keras.optimizers.sgd(lr=1e-3,decay=0, momentum=0.9, nesterov=True)\n",
        "adam=keras.optimizers.adam(lr=1e-3)\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=3, min_lr=1e-20,mode='min',verbose=1)\n",
        "early_stopping=keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,verbose=1,mode='min',min_delta=0.01,restore_best_weights=True)\n",
        "model.compile(optimizer=adam,loss='mean_squared_error') #,metrics=['mse']\n",
        "step_size_train=train_generator.n//train_generator.batch_size #It should typically be equal to the number of samples of your dataset divided by the batch size.\n",
        "step_size_valid=validation_generator.n//validation_generator.batch_size\n",
        "history=model.fit_generator(train_generator,steps_per_epoch=step_size_train,epochs=50,validation_data=validation_generator,validation_steps=step_size_valid,callbacks=[reduce_lr]) #,early_stopping\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz2FLxagH-Y3",
        "colab_type": "code",
        "outputId": "e92bfe5e-019e-49d1-98c5-d62531d1069d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "\n",
        "for e,v in enumerate(history.history['val_loss']):\n",
        "  print(\"Epoch \",e,\":\",v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 : 1.8563402223587036\n",
            "Epoch  1 : 0.5814175848166148\n",
            "Epoch  2 : 0.5854243016242981\n",
            "Epoch  3 : 1.3972849249839783\n",
            "Epoch  4 : 0.5843948904673258\n",
            "Epoch  5 : 0.39702599167823793\n",
            "Epoch  6 : 0.3186376744508743\n",
            "Epoch  7 : 0.3412351727485657\n",
            "Epoch  8 : 0.3250936098893483\n",
            "Epoch  9 : 0.3429074364900589\n",
            "Epoch  10 : 0.3596832213799159\n",
            "Epoch  11 : 0.3477966129779816\n",
            "Epoch  12 : 0.3110671037435532\n",
            "Epoch  13 : 0.34113820413748425\n",
            "Epoch  14 : 0.29659178654352825\n",
            "Epoch  15 : 0.3261910027265549\n",
            "Epoch  16 : 0.32947644909222923\n",
            "Epoch  17 : 0.30716048419475556\n",
            "Epoch  18 : 0.31829468508561454\n",
            "Epoch  19 : 0.3118597435951233\n",
            "Epoch  20 : 0.31129716416200004\n",
            "Epoch  21 : 0.3089953891436259\n",
            "Epoch  22 : 0.30766829252243044\n",
            "Epoch  23 : 0.30799841503302255\n",
            "Epoch  24 : 0.3080869122346242\n",
            "Epoch  25 : 0.30959687848885853\n",
            "Epoch  26 : 0.3076077095667521\n",
            "Epoch  27 : 0.30686045746008556\n",
            "Epoch  28 : 0.3069929172595342\n",
            "Epoch  29 : 0.30866892337799073\n",
            "Epoch  30 : 0.3106473763783773\n",
            "Epoch  31 : 0.30929792563120523\n",
            "Epoch  32 : 0.30908912340799966\n",
            "Epoch  33 : 0.3081625517209371\n",
            "Epoch  34 : 0.308074032664299\n",
            "Epoch  35 : 0.3081503647565842\n",
            "Epoch  36 : 0.3093271102507909\n",
            "Epoch  37 : 0.3104432300726573\n",
            "Epoch  38 : 0.3097878297170003\n",
            "Epoch  39 : 0.31024137198925017\n",
            "Epoch  40 : 0.30942277669906615\n",
            "Epoch  41 : 0.3097557765245438\n",
            "Epoch  42 : 0.30903478026390074\n",
            "Epoch  43 : 0.30801802217960356\n",
            "Epoch  44 : 0.3086781543493271\n",
            "Epoch  45 : 0.30895721952120464\n",
            "Epoch  46 : 0.30869825104872384\n",
            "Epoch  47 : 0.3089770839611689\n",
            "Epoch  48 : 0.30736751437187193\n",
            "Epoch  49 : 0.3072200403610865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWp83iCg6ZnP",
        "colab_type": "code",
        "outputId": "1a897f8f-48c9-429a-ed22-d4494534cbdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcXGWd7/HPr7au7vSShIQkZCEY\nYCTIatjEGRGFKwhmZkQRAZVxREfnKqMzd9QZ9aVe5977mhmdqzIqI44wwyDIJo542WQAFYEQwhZA\nQ9gSshGSXtJrVf3uH8+p6upOV3d1uqs7Xef7fr3qVVWnTp3znOrq863nec45j7k7IiIiAInpLoCI\niOw/FAoiIlKiUBARkRKFgoiIlCgURESkRKEgIiIlCgWRKpnZD83sf1Y57wtm9vaJLkdkqikURESk\nRKEgIiIlCgWpK1GzzV+Z2eNmtsfMrjSzBWb2czPrNLO7zGxO2fzvMrOnzGy3mf2XmR1R9tpxZrY2\net91QHbYus4xs3XRe39tZkfvY5k/YmYbzOw1M7vVzA6KppuZfcPMtptZh5k9YWZviF4728zWR2Xb\nbGZ/uU8fmMgwCgWpR+8GzgAOB84Ffg58HphP+M5/EsDMDgeuBS6LXrsN+KmZZcwsA9wC/BswF/hx\ntFyi9x4H/AD4KHAA8D3gVjNrGE9Bzex04H8B7wUWAS8CP4pePhP4g2g72qJ5dkavXQl81N1bgDcA\nvxjPekUqUShIPfqWu29z983A/cCD7v6ou/cCNwPHRfOdD/zM3e909wHgH4BG4E3AyUAa+Cd3H3D3\nG4CHy9ZxKfA9d3/Q3fPufhXQF71vPC4EfuDua929D/gccIqZLQcGgBbg9YC5+9PuviV63wCw0sxa\n3X2Xu68d53pFRqRQkHq0rexxzwjPm6PHBxF+mQPg7gXgZWBx9NpmH3rFyBfLHh8MfCZqOtptZruB\npdH7xmN4GboItYHF7v4L4NvA5cB2M7vCzFqjWd8NnA28aGb3mtkp41yvyIgUChJnrxB27kBowyfs\n2DcDW4DF0bSiZWWPXwa+5u6zy25N7n7tBMswi9ActRnA3b/p7m8EVhKakf4qmv6wu68GDiQ0c10/\nzvWKjEihIHF2PfBOM3ubmaWBzxCagH4NPADkgE+aWdrM/hg4sey9/wJ8zMxOijqEZ5nZO82sZZxl\nuBa4xMyOjfoj/o7Q3PWCmZ0QLT8N7AF6gULU53GhmbVFzV4dQGECn4NIiUJBYsvdnwUuAr4FvEro\nlD7X3fvdvR/4Y+BDwGuE/oebyt67BvgIoXlnF7Ahmne8ZbgL+AJwI6F2sgJ4X/RyKyF8dhGamHYC\nfx+9djHwgpl1AB8j9E2ITJhpkB0RESlSTUFEREoUCiIiUqJQEBGREoWCiIiUpKa7AOM1b948X758\n+XQXQ0RkRnnkkUdedff5Y80340Jh+fLlrFmzZrqLISIyo5jZi2PPpeYjEREpo1AQEZEShYKIiJQo\nFEREpEShICIiJQoFEREpUSiIiEhJbELhma0d/OMdz/Lanv7pLoqIyH4rNqGwcccevvWLDWzv7J3u\nooiI7LdiEwqNmSQA3f35aS6JiMj+Kz6hkA6h0KtQEBGpKDah0KSagojImGITCsWaQs+AQkFEpJLY\nhEJWoSAiMqbYhEKx+ahHzUciIhXFJhSKRx+ppiAiUllsQiGbUkeziMhYYhMKiYSRTSfoVU1BRKSi\n2IQChCOQ1KcgIlJZrEKhKZNS85GIyChiFQpqPhIRGV2sQqExk6S7PzfdxRAR2W/FKhSa0ikdkioi\nMopYhUI2k6RnoDDdxRAR2W/VLBTMbKmZ3WNm683sKTP71AjznGZm7Wa2Lrp9sVblAWhKJ+lR85GI\nSEWpGi47B3zG3deaWQvwiJnd6e7rh813v7ufU8NylDRmkmo+EhEZRc1qCu6+xd3XRo87gaeBxbVa\nXzWy6SQ9/Wo+EhGpZEr6FMxsOXAc8OAIL59iZo+Z2c/N7MgK77/UzNaY2ZodO3bsczmaMmo+EhEZ\nTc1DwcyagRuBy9y9Y9jLa4GD3f0Y4FvALSMtw92vcPdV7r5q/vz5+1yWxnRoPnL3fV6GiEg9q2ko\nmFmaEAjXuPtNw1939w5374oe3wakzWxercrTmElScOjLqQlJRGQktTz6yIArgafd/esV5lkYzYeZ\nnRiVZ2etylQap1mdzSIiI6rl0UenAhcDT5jZumja54FlAO7+XeA84M/MLAf0AO/zGrbtlI+pMLtW\nKxERmcFqFgru/kvAxpjn28C3a1WG4Yqjr+mieCIiI4vXGc1pDckpIjKaWIVCk4bkFBEZVaxCoVE1\nBRGRUcUqFErNR6opiIiMKFahUGo+Uk1BRGREsQqFRvUpiIiMKl6hoD4FEZFRxSsUVFMQERlVrEIh\nk0yQMNUUREQqiVUomBlNmZTOaBYRqSBWoQDRQDtqPhIRGVHsQqExk9BVUkVEKohdKDSlU3Rr9DUR\nkRHFLhSymSQ9AxpkR0RkJLELhaa0xmkWEakkdqHQmFFHs4hIJfELhXRS5ymIiFQQv1DIKBRERCqJ\nXyjoPAURkYpiFwpNmaTOaBYRqSB2oZBNJ+nLFSgUfLqLIiKy34ldKBSvlNqbU21BRGS42IVCcfQ1\nNSGJiOwtdqGQ1UA7IiIVxS4UiqOv6aJ4IiJ7i10oqPlIRKSy2IVCaZxm1RRERPYSv1DIqE9BRKSS\n+IaCagoiInupWSiY2VIzu8fM1pvZU2b2qRHmMTP7ppltMLPHzez4WpWnqFFHH4mIVJSq4bJzwGfc\nfa2ZtQCPmNmd7r6+bJ6zgMOi20nAd6L7minWFLpVUxAR2UvNagruvsXd10aPO4GngcXDZlsNXO3B\nb4DZZraoVmWCskNSVVMQEdnLlPQpmNly4DjgwWEvLQZeLnu+ib2DAzO71MzWmNmaHTt2TKgsxVDQ\nIakiInureSiYWTNwI3CZu3fsyzLc/Qp3X+Xuq+bPnz+h8qSSCTLJhDqaRURGUNNQMLM0IRCucfeb\nRphlM7C07PmSaFpNZdMJndEsIjKCWh59ZMCVwNPu/vUKs90KfCA6CulkoN3dt9SqTEVNmRTd/bla\nr0ZEZMap5dFHpwIXA0+Y2bpo2ueBZQDu/l3gNuBsYAPQDVxSw/KUNGaS9AwUpmJVIiIzSs1Cwd1/\nCdgY8zjwiVqVoZLGdJIe1RRERPYSuzOaoVhTUJ+CiMhw8QyFdFJnNIuIjCCeoZBJ6jwFEZERxDMU\n0kkdkioiMoL4hMKenfD8fdDfHZqPFAoiInuJTyg8fy9cdS7sflHNRyIiFcQnFLJt4b5nN40ZNR+J\niIwkRqEwO9z3ttOUTjKQdwbyOoFNRKRcjEIhqin0tmv0NRGRCuITCo2DNYWsxlQQERlRfEKhoTXc\n97bTlNGYCiIiI4lPKKQykG6C3t2D4zSr+UhEZIj4hAKEfoXe3YPjNKumICIyRAxDoX1wnGbVFERE\nhohZKMweevSRagoiIkPELBTaoGf3YEezagoiIkPELxR0SKqISEWxDIWmTBhwTuM0i4gMFa9QaJwN\nfR00psIooRqnWURkqHiFQrYNvEBDoRvQeQoiIsPFLxSARF97NCSnmo9ERMrFMhSKh6WqpiAiMlR8\nQyGdpKdffQoiIuViFgqDV0oNNQU1H4mIlItZKAyvKaj5SESkXDxDIRqSUxfEExEZKp6hENUUdEE8\nEZGh4hUKiWQYbKfYfKRQEBEZIl6hAGWXulDzkYjIcDENhd1kM2o+EhEZrqpQMLNPmVmrBVea2Voz\nO3OM9/zAzLab2ZMVXj/NzNrNbF10++K+bMC4FWsKadUURESGq7am8Cfu3gGcCcwBLgb+9xjv+SHw\njjHmud/dj41uX6myLBNTHH0tOqPZ3adktSIiM0G1oWDR/dnAv7n7U2XTRuTu9wGvTaBstRGNvpZN\nJ3GHvpzOahYRKao2FB4xszsIoXC7mbUAk7E3PcXMHjOzn5vZkZVmMrNLzWyNma3ZsWPHxNZY1tEM\nGpJTRKRcqsr5PgwcC2x0924zmwtcMsF1rwUOdvcuMzsbuAU4bKQZ3f0K4AqAVatWTay9J9sGfR00\nRVveM5BnzoQWKCJSP6qtKZwCPOvuu83sIuBvgfaJrNjdO9y9K3p8G5A2s3kTWWZVohPYWiyMqaDO\nZhGRQdWGwneAbjM7BvgM8Bxw9URWbGYLzcyixydGZdk5kWVWpTFcFK/FQyjosFQRkUHVNh/l3N3N\nbDXwbXe/0sw+PNobzOxa4DRgnpltAr4EpAHc/bvAecCfmVkO6AHe51NxKFBUU5hFF6DR10REylUb\nCp1m9jnCoai/b2YJoh18Je5+wRivfxv4dpXrnzxRKDQX9gCm5iMRkTLVNh+dD/QRzlfYCiwB/r5m\npaqlKBSy+U5ARx+JiJSrKhSiILgGaDOzc4Bed59Qn8K0iUKhMR+aj/apT+G+v4ef//VklkpEZL9Q\n7WUu3gs8BLwHeC/woJmdV8uC1Uw0+lpDFAr71Hz02zvgmdsms1QiIvuFavsU/gY4wd23A5jZfOAu\n4IZaFaxmMs1gCTIDUfPRvtQUOrdC11ZwBxv1xG4RkRml2j6FRDEQIjvH8d79SyIBDa2kBzoA6Okf\n5zjN7iEQ8v3Qs6sGBRQRmT7V1hT+n5ndDlwbPT8fmLntJ9k2En3tJBM2/ppCz64QCBBqDE1zJ798\nIiLTpKpQcPe/MrN3A6dGk65w95trV6waa5yNFUdf6x/nJZw6t5Y93gILVk5u2UREplG1NQXc/Ubg\nxhqWZeoMuXz2OJuPuspCoWvb5JZLRGSajRoKZtYJjHSWsQHu7q01KVWtZdvg1Q1RTWGczUfDawoi\nInVk1FBw95apKsiUiobk3KdxmouhkMpCp2oKIlJfZuYRRBNVNtDOuDuaO7dCQyu0LVVNQUTqTtV9\nCnUlOxsGumlOFcZ/RnPXVmhZCM0L1KcgInUnpjWFcKmLA1K9+9Z81LwgBEN5/4KISB2IdSjMSfbs\nW/NRy6LBUJiCq32LiEyVeIeC7Rnf0UfuUSgsgOaFkO+D3t01KqSIyNSLZyhEo6/NTnSPr6bQuzsE\nQbGmAGpCEpG6Es9QiGoKrXSPr6ZQPAS12KcACgURqSuxDoUW20NfrkC+UGW/QPEQ1JZFofkIFAoi\nUldiHQphSM5xDLRTPAS1ZWHoV4Chl70QEZnh4hkK6SZIpJnlYaCdqvsVijWF5gXQ0BLGZtBZzSJS\nR+IZCmaQbaOpEIVCtf0Kndsg0wINzeF5y0Kd1SwidSWeoQCQbSOb34eaQrGDGUK/gs5qFpE6EvNQ\nCENyVn1Wc9e2oaGgmoKI1JlYh0JpnOaqm4+2hP6EopaFoUlJZzWLSJ2Ibyg0zi6N01zV0UfuIQCG\n1xRyPdDbXqNCiohMrfiGQraNVP84mo9620MADO9TAPUriEjdiHUoJPtDTaGqjubSOQqLBqfprGYR\nqTOxDgXL99FAPz39VYzTXNzxD+9TKH9NRGSGi3EohIvitbKnuppCccdfXlNo1lnNIlJfahYKZvYD\nM9tuZk9WeN3M7JtmtsHMHjez42tVlhEVL4pn3fT0F8aev7jjbymrKTS0QHqWagoiUjdqWVP4IfCO\nUV4/Czgsul0KfKeGZdlbVFM4INlD90CVzUeZ5hAERWYhJBQKIlInahYK7n4f8Noos6wGrvbgN8Bs\nM1s0yvyTK6opzEv10lvN0UfFYTiHa1mkUBCRujGdfQqLgZfLnm+Kpu3FzC41szVmtmbHjh2Ts/bi\nOM3JnuoOSS0Owzlc8wL1KYhI3ZgRHc3ufoW7r3L3VfPnz5+chUajrx2QqnL0ta6tQ/sTiloW6axm\nEakb0xkKm4GlZc+XRNOmRkMrALOtZ+wzmktjM49QU2hZAAN7oK+zBoUUEZla0xkKtwIfiI5COhlo\nd/epu7pcOgupLLMT3WM3H/V1wkB35T4F0FnNIlIXUrVasJldC5wGzDOzTcCXgDSAu38XuA04G9gA\ndAOX1KosFWXbaPMqzlMonaOwcO/XikHRuQXmHTa55RMRmWI1CwV3v2CM1x34RK3WX5XsbFp69ox9\nldSuUUKhdFazagoiMvPNiI7mmsm20VJVTSHa4TePFgoaV0FEZr7Yh0KTV1FTKO7wR6opNLRCqlF9\nCiJSFxQKha4qmo+2Qbpp6NnMRWbRYDs6V0FEZr7Yh0JjvquK5qNobGazkV9XKIhInYh3KDTOJpvr\nJFcoMJAf5aJ4ndtG7k8oalmos5pFpC7EOxSybSTI00Tf6OcqFGsKlTSrpiAi9SH2oQBhTIVRz2ru\n2jZ6KLQshP4undUsIjOeQoEwpkLFmkJfZ9jhjxUKoHMVRGTGi3kolI2+VikURjtHoUgjsIlInYh5\nKISaQpuNcgLbaOcoFBWvf6R+BRGZ4RQKQCvdlWsKxZPSRg2F4vWPFAoiMrPFPBSi5iMbZUyF0S6G\nV76cVFbNRyIy48U8FMKYCq3sobu/wjjNnVvCZSyi8RdGZBb6FVRTEJEZLt6hkEzj6VnMTnRz329f\nHXmerm2heajS2cxFGqtZROpAvEMBsMbZnLAwwY1rN/GbjTv3nqHSiGvDtaimICIzX+xDgWwbR86B\nJXMa+dtbnqQ/N+xyF51bRx5xbbiWRbpSqojMeAqFbBvJ/na+uvoNbNjexb/cv3Ho69XWFJoXQF8H\n9O+pTTlFRKaAQiE7G3rbeevrD+SsNyzkm3f/jpd2dofX+rqgv3PwkNPRlM5qVhOSiMxcCoVsG/Tu\nBuCL564klTC+8JMncfeycxSq6VOIQkFNSCIygykUsm3Q2w7AorZGPn3m73Hvb3dw2xNbB3/1V9On\n0KxhOUVk5lMoZNugtwMKoYP5g6cczJEHtfLlnz5Fz2ubwjzjqSnoongiMoMpFLJtgIe+AyCVTPC1\nPzqKHV193PvIk2GeavoUGudAskE1BRGZ0RQKjeFSF8UmJIBjl87mopMO5sUXN1JINpQuhzEqsxAe\n6lMQkRlMoRBdFI+e3UMm/+V/+z2WpTt4Jd/G+i1VDp6jEdhEZIZTKBRDoaymANDWmOYtB+XZaXM5\n/4oHeOj518ZeVotCQURmttR0F2DaFUNh00OQ6wt9C31d0L+HpvYNHH7oCczf2sDFVz7Idy46ntNf\nP0r/QstCeP7eqSm3iEgNKBRmHRju7/7KiC83HnIyP159Ch/614f5yNWP8A/vOZo/Om7JyMuac0io\ncez4Lcw/vEYFFhGpHXP36S7DuKxatcrXrFkzuQt96cEwDnOmGRqao/sWyMyCVAMAnb0DXHr1Izyw\ncSdfOncll5x6yN7L6doB31gJx38A3vmPk1tGEZEJMLNH3H3VWPOppgCw7KQxZ2nJpvnXS07gk9c+\nypd/up6t7b18+szDaUglB2dqng9vOA/WXQunf2HwyCYRkRlCHc3jkE0n+ecLj+f9Jy3je/dt5J3f\n/CUPvzCsA/qkj8LAHnj036enkCIiE1DTUDCzd5jZs2a2wcw+O8LrHzKzHWa2Lrr9aS3LMxlSyQR/\n90dH8a+XnEBPf573fPcB/ubmJ+joHQgzHHQsLDsFHroCChWG+BQR2U/VLBTMLAlcDpwFrAQuMLOV\nI8x6nbsfG92+X6vyTLa3/t6B3PEXf8CfnHoI1z70Emd8/V5ufyo6HPWkj8LuF+G3t09vIUVExqmW\nNYUTgQ3uvtHd+4EfAatruL4pN6shxRfPXcnNHz+VOU0ZPvpvj/CpHz2Kv/4caF0MD35nuosoIjIu\ntQyFxcDLZc83RdOGe7eZPW5mN5jZ0pEWZGaXmtkaM1uzY8eOWpR1Qo5ZOpuf/vc389G3vI6frHuF\nX21shxP+FJ6/D7atn+7iiYhUbbo7mn8KLHf3o4E7gatGmsndr3D3Ve6+av78+VNawGqlkwk+fcbh\nzGvO8MNfPw/HfxBSWXjoe9NdNBGRqtUyFDYD5b/8l0TTStx9p7v3RU+/D7yxhuWpuYZUkvefuIy7\nn9nOS72NcNR74LHroLuKS2SIiOwHahkKDwOHmdkhZpYB3gfcWj6DmZUPVPAu4OkalmdKXHjywSTN\nuPqBF+Ckj0GuB9ZePd3FEhGpSs1Cwd1zwJ8DtxN29te7+1Nm9hUze1c02yfN7Ckzewz4JPChWpVn\nqixozXLWUYu4bs3L7Jnzelj++/Dw9yGfm+6iiYiMqaZ9Cu5+m7sf7u4r3P1r0bQvuvut0ePPufuR\n7n6Mu7/V3Z+pZXmmyofetJzO3hw3Pbo5HJ7a/jI8+7PpLpaIyJimu6O5Lh2/bDZHL2njh796Hj/8\nLGhbBg+qw1lE9n8KhRowMz70puU8t2MPv9y4C078CLz4K3jq5ukumojIqBQKNfLOoxcxrznDVb9+\nAU74MCw9CW74MDxxw3QXTUSkIoVCjZQfnvpiJ3DRjeGaSDd9BNb9x9QVxB06t0F+YOrWKSIzli6d\nXUMXnnww//xfz3H1Ay/yhXNWwoU/hh9dALd8PIzytuqSyV9pxyuweS28sja6fxR6d0MiBbMPhgMO\njW6vC/dLT4J04+SXQ0RmJIVCDS1ozXL2UYu4/uGX+fQZhzOroQkuuA6uvxj+87Lw6/2kS4e+ac+r\n8Nw9sPEesAQcdFy48uqBR0I6O3Te/m7Ysg5efgg2PQyb1kBXdFE+S8KClbByNRy4EvZsh50bYOdz\n4fIbuZ4wX9M8OPHScFmOWQdU3pidz8Hj10FvB7zpz6GtwuhzIjKjaeS1Glv70i7++J9/zVdXH8nF\npywPE3N98ONLwmGqZ3wFlpwIz90NG+6CV9YBDo1zw7w90dnQiRQceAQsOjZcPmPTw7DtSShE5z/M\nOQSWrILFbwy3hUdVrgEUCtC5Jbz/4Svhd7dDqhGOuxBO/jgcsCLM19seOsfXXQsv/wawUI5EMsz3\n5ssGx7gezj3UVp75WTije6A7uvWEMMv1hJrKirfBitOhZZSxr0VkwqodeU2hUGPuzurLf8Wevhx3\n/sVbSCQsvJAfgBv/FNbfEp5bApacAIe+HQ59W9j5WyKc4/DKoyEstqwL9/l+WHx8mL94mzVv3wu5\n/Rl44Fvw+PWhXEecA8kGeOY/IdcL8w6HYy6Ao88Hz8PdX4UnroemA+Atnw3NYMl0WFb7plCjeOxH\n8OpvQ4g0zoF0U3RrDMOcJtOw5XHofjW8b8EbQjisOB1aFsJrz8NrG2HX8+HxruehZ1cIy1nzwrqL\nt8Y5YT2WKLtZuCUzYVtS0S2ZGbxPJKOQSw+GnSUAD6EW/oBDn4eJ5X9gKAyEz62QL3ucC8tLRutK\npqN1Z8rKmhwsb6JsBL/yZQN4YXC5+YGhj/P90a34uC88Li5z+PaV1hN9D83CY7NQnuJnULy3ZDQP\nQ99XkUflzUW3/LDH+fAdKp/mhWF/u7JbcXke3Rf/FsPLO6ScXvYnKvvbJYYvv3xbk+H1RGpwuV6o\ncBvtuzHsM8WGlaPsHsLfJhn9jZLRLZEu2xYbutx0E2Rbx/gbjEyhsB+55dHNXHbdOo5a3MYn3noo\nZ65cEMIhn4N114Rf2697S9i5jaX497Kx/jn3QefWcD7FmisBg6POg2PeHwJo+PpeeRTu+AK8cD/M\nXRHGpX7ubnj+fsBh2ZvgmPNh5R9WHpa0UIBtT8CGu+G5X8BLvwk7vHINbTD3kHBrnBuCoXvn4G3P\nq3u/R6RenXoZnPHlfXqrQmE/4u78eM0mLv+vDby4s5vDDmzm429dwblHH0QquR8eAFY8Uqn4678S\nd/jdHXDnF2HHMzD3dXD0++Do94ad+Hj1dYXzOfo6Q3PY3ENCUI4WgO6hWar4i9ML0fRCmFb8JZ3r\nDc12xceFXAjlQm7Yr9Z89AvVhv3io3I5EmW/8BLJ6HGqbP0D0S/4fsj1R7+U8yP/CmX4uqL1J1KD\nNY5EanB9qcxgbahYG0lmAI+2caDsV/rA0F+6Q2pEhahc0X3x8/Oy0QP3qi1V+jyKNZRU2a/wVNn0\nZNkv8lTYvmJNoHQrq0FgZbW/6Hn5PIWy95R/ZsM/y/Lllz7/sm0ur8VUqr2Ul6W0Lso+i2ItxYd+\nzkO+T2Xzl/5GA9H3Marxjfh5AwuPDs3E+0ChsB/K5Qv87IktXH7PBn67rYtlc5v42FtWsPrYg5jV\nMIP7/PM5aH8p7MhrUYMRkQlTKOzHCgXnrqe3cfk9G3hsUzuZVIJTVxzA21cu4O1HLGBBa3bshYiI\njINCYQZwdx5+YRe3P7WVO9dv46XXugE4Zkkbbz9iASceMpcVBzZzwKwMpl/gIjIBCoUZxt353fYu\n7ly/jbue3sajL+0uvdbWmGbF/FkcemAzK+Y3M6+5gVyhQH/eGcgVGMgX6M8VaMwkOWPlAg4+YNY0\nbomI7I8UCjPcjs4+1m/p4LntXTy3I9w2bN/Dq119Y773uGWzWX3MQZxzzEHMa26YgtKKyP5OoVCn\n2rsHeK27n0wqQTppZJKJ6HGC7Z19/PSxV7jl0c08s7WTZMI49dB5nHP0Ig5f0MKC1gbmNzfsn0c8\niUhNKRRi7tmtnfxk3WZ+su4VNu/uKU1PGMxrbmBBa5YFrVnaGtM0NySZ1ZCiOZuiuSHFrEyKhnSC\nhBkJI7o3kgkjlTSaG1K0ZFPhPdH8pZPyCE1huYKTL4T7IqPs6E6MTCpBMqG+EpGpoFAQIOygn9na\nyeZdPWzr7GVbey9bO3rZ1tHHto5eOnoG6OrLsac/T76w79+FhlQCd8gVCoxnMZlkgoZ0gsZ0kmw6\nSWM6SWtjigNbssxvaSjdDmxpoLUxjbtTcMgXnELBybvjDo2ZJE2ZJE2ZFLMyyeh5OMw3X3AKHm7h\nfWHdligGVQg/w0Y9ojaZMFIJm9RO//LtMWPM5ffl8nT25ujoGaC7P082naQ1m6IlmyabTuiABKmo\n2lCYwQfHSzXMjCMWtXLEotFPjXd3+nKFEBB9OfpzhcGdrxdv0J8rsKcvR2dfjq7eXOlx70CehIWd\nZjJhpJNGMpEgmQg7W8cHz5MinJfTnyvQm8vT05+nL7rvGcjT3jPA01s7uO93fXT27n9jWyejbUwl\njKQZzuDn5A756PMyQi0rXHFjsNZV8BBM+SikyiUsXHa9IZ2gIZWgIZUkYdDVl6OjN/xdKkklrFTb\nS5iRL6ut5QsF8gUvv0BH6XSKuI7XAAAHHElEQVSrYpB4VH4vPh42b8KsdO6VWeUALb1OsWYY5i2v\ndRavQpIozbf3woo1zlx+cBuKj4ufdSEqZ/E5DF3u0DIPBn/5NiSjv2Miuk8mjEQinNOWj354FH+A\nFP9eBiQSNuRHBWU/KoavL2xPdB99suX/D+XPh3yW5Ve5AD5wysH8+emHjfzBTxKFggDhi52Nfq3v\nT53TPf15Xu3qY3tnLx09udI/biJBqUnLgJ6BPN39ebr7c+G+LzwHSCYYfF/0zw/lO8HiDiY8thHO\n1C1EO4bcsB1truCDzWwJG9LkVnpfdHKrRzuV4nzJBCE4o8fu0J8v0Jcr0DeQD/e5sI7mhhSt2RSt\njWlasqH5rimToncg1Bw6e3N09Q2E+94cDoMhnRzc2Q3fQRW5e2knOXTHSVlQhO0pzj+S4nzFz7T4\nHEIQOuHzKO7EizWlSlLJsA2pZKL0gyOVsNJnWNzxJ8rbJr1sHdHfAC8Lu2HbU6pBRvf5wuDnEf5G\ng82nibK9dLHsxW0a/EiK2zb4OZQ3m1JW1MHF2bDnw/9G4cmK+c2VP6xJolCQ/VpjJsnSuU0snds0\n3UURiQUdhiIiIiUKBRERKVEoiIhIiUJBRERKFAoiIlKiUBARkRKFgoiIlCgURESkZMZd+8jMdgAv\n7uPb5wGvTmJxZpK4bru2O1603ZUd7O7zx1rQjAuFiTCzNdVcEKoexXXbtd3xou2eODUfiYhIiUJB\nRERK4hYKV0x3AaZRXLdd2x0v2u4JilWfgoiIjC5uNQURERmFQkFEREpiEwpm9g4ze9bMNpjZZ6e7\nPLViZj8ws+1m9mTZtLlmdqeZ/S66nzOdZawFM1tqZveY2Xoze8rMPhVNr+ttN7OsmT1kZo9F2/3l\naPohZvZg9H2/zswy013WWjCzpJk9amb/GT2v++02sxfM7AkzW2dma6Jpk/Y9j0UomFkSuBw4C1gJ\nXGBmK6e3VDXzQ+Adw6Z9Frjb3Q8D7o6e15sc8Bl3XwmcDHwi+hvX+7b3Aae7+zHAscA7zOxk4P8A\n33D3Q4FdwIensYy19Cng6bLncdnut7r7sWXnJkza9zwWoQCcCGxw943u3g/8CFg9zWWqCXe/D3ht\n2OTVwFXR46uAP5zSQk0Bd9/i7mujx52EHcVi6nzbPeiKnqajmwOnAzdE0+tuuwHMbAnwTuD70XMj\nBttdwaR9z+MSCouBl8ueb4qmxcUCd98SPd4KLJjOwtSamS0HjgMeJAbbHjWhrAO2A3cCzwG73T0X\nzVKv3/d/Av4HUIieH0A8ttuBO8zsETO7NJo2ad/z1ERLJzOLu7uZ1e1xyGbWDNwIXObuHeHHY1Cv\n2+7ueeBYM5sN3Ay8fpqLVHNmdg6w3d0fMbPTprs8U+zN7r7ZzA4E7jSzZ8pfnOj3PC41hc3A0rLn\nS6JpcbHNzBYBRPfbp7k8NWFmaUIgXOPuN0WTY7HtAO6+G7gHOAWYbWbFH331+H0/FXiXmb1AaA4+\nHfi/1P924+6bo/vthB8BJzKJ3/O4hMLDwGHRkQkZ4H3ArdNcpql0K/DB6PEHgZ9MY1lqImpPvhJ4\n2t2/XvZSXW+7mc2PagiYWSNwBqE/5R7gvGi2uttud/+cuy9x9+WE/+dfuPuF1Pl2m9ksM2spPgbO\nBJ5kEr/nsTmj2czOJrRBJoEfuPvXprlINWFm1wKnES6luw34EnALcD2wjHDZ8fe6+/DO6BnNzN4M\n3A88wWAb8+cJ/Qp1u+1mdjShYzFJ+JF3vbt/xcxeR/gFPRd4FLjI3fumr6S1EzUf/aW7n1Pv2x1t\n383R0xTwH+7+NTM7gEn6nscmFEREZGxxaT4SEZEqKBRERKREoSAiIiUKBRERKVEoiIhIiUJBZAqZ\n2WnFK3qK7I8UCiIiUqJQEBmBmV0UjVOwzsy+F110rsvMvhGNW3C3mc2P5j3WzH5jZo+b2c3Fa9mb\n2aFmdlc01sFaM1sRLb7ZzG4ws2fM7Borv0CTyDRTKIgMY2ZHAOcDp7r7sUAeuBCYBaxx9yOBewln\niwNcDfy1ux9NOKO6OP0a4PJorIM3AcWrWB4HXEYY2+N1hOv4iOwXdJVUkb29DXgj8HD0I76RcIGx\nAnBdNM+/AzeZWRsw293vjaZfBfw4uj7NYne/GcDdewGi5T3k7pui5+uA5cAva79ZImNTKIjszYCr\n3P1zQyaafWHYfPt6jZjya/Hk0f+h7EfUfCSyt7uB86Lr1RfHvz2Y8P9SvALn+4Ffuns7sMvMfj+a\nfjFwbzT62yYz+8NoGQ1m1jSlWyGyD/QLRWQYd19vZn9LGN0qAQwAnwD2ACdGr20n9DtAuFTxd6Od\n/kbgkmj6xcD3zOwr0TLeM4WbIbJPdJVUkSqZWZe7N093OURqSc1HIiJSopqCiIiUqKYgIiIlCgUR\nESlRKIiISIlCQUREShQKIiJS8v8BTy5HifycJiwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0KUOO68Zhe",
        "colab_type": "code",
        "outputId": "02949a65-f7f4-4e20-ae2e-944f15efd136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Evaluating the model\n",
        "test_result=[]\n",
        "q_est,r_est=[],[]\n",
        "with open(os.path.join(dataset_root,'real.json'),'r') as f_eval:\n",
        "  test_list=json.load(f_eval)\n",
        "\n",
        "for img in test_list:\n",
        "  img_path=os.path.join(dataset_root,'images','real',img['filename'])\n",
        "  img_arr=image.load_img(img_path,target_size=(299,299))\n",
        "  x=image.img_to_array(img_arr)\n",
        "  x=preprocess_input(x)\n",
        "  x=np.expand_dims(x,axis=0)\n",
        "  output=model.predict(x)\n",
        "  output=output.tolist()\n",
        "  test_result.append({'filename':img['filename'],'q':output[:4],'r':output[4:]})\n",
        "  q_est.append(output[0][:4])\n",
        "  r_est.append(output[0][4:])\n",
        "  print(output)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.06348899006843567, 0.06687120348215103, 0.13640759885311127, 0.03139131888747215, 0.10568525642156601, 0.11914920061826706, 3.251842737197876]]\n",
            "[[0.059174854308366776, -0.02354532666504383, 0.057957861572504044, 0.018545297905802727, -0.07661950588226318, 0.005956687498837709, 3.2915780544281006]]\n",
            "[[0.04756024852395058, 0.03099094144999981, 0.04384845867753029, -0.018188100308179855, -0.15903426706790924, 0.0008900674292817712, 4.8843255043029785]]\n",
            "[[0.0011834509205073118, -0.02057334966957569, 0.02484503760933876, 0.01572663150727749, -0.05504092574119568, -0.009950249455869198, 4.693317890167236]]\n",
            "[[0.07476510107517242, 0.07751984149217606, -0.04094855114817619, -0.02761460468173027, -0.16005922853946686, -0.13956931233406067, 4.939939975738525]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIiK4fcvBPr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# img_path=os.path.join(dataset_root,'images','test','img000014.jpg')\n",
        "# img_arr=image.load_img(img_path,target_size=(299,299))\n",
        "# x=image.img_to_array(img_arr)\n",
        "# x=preprocess_input(x)\n",
        "# x=np.expand_dims(x,axis=0)\n",
        "# output=model.predict(x)\n",
        "# output=output.tolist()\n",
        "# output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIM3gs2Sn4Zo",
        "colab_type": "text"
      },
      "source": [
        "**Extracting r_gt and q_gt from real.json**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ojs_fgUl6cY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_json=[]\n",
        "q_gt,r_gt=[],[]\n",
        "with open(os.path.join(dataset_root,'real.json'),'r') as f:\n",
        "  real_json=json.load(f)\n",
        "for i in range(len(real_json)):\n",
        "  q_gt.append(real_json[i]['q_vbs2tango'])\n",
        "  r_gt.append(real_json[i]['r_Vo2To_vbs_true'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwJQ8wnhv-y-",
        "colab_type": "text"
      },
      "source": [
        "**Calculating Orientation Score.**\n",
        "\n",
        "Orientation score is the angle of the rotation, that aligns the estimated and ground truth orientations:\n",
        "\n",
        "score(i)orientation=2⋅arccos(∣∣⟨q(i)est,q(i)gt⟩∣∣)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMhDPt4loE0z",
        "colab_type": "code",
        "outputId": "ba6a38e1-75b9-47bd-e7d1-2521dd70bce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import math\n",
        "score_orientation=0\n",
        "bra_ket=[]\n",
        "for i in range(len(real_json)):\n",
        "  bra_ket.append(q_est[i][0]*q_gt[i][0]+q_est[i][1]*q_gt[i][1]+q_est[i][2]*q_gt[i][2]+q_est[i][3]*q_gt[i][3])\n",
        "  \n",
        "  \n",
        "for i in range(len(real_json)):\n",
        "  if bra_ket[i]<1:\n",
        "    score_orientation+=2*math.acos(bra_ket[i])\n",
        "score_orientation\n",
        "#bra_ket\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.574424121993307"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1nAYXDswRCT",
        "colab_type": "text"
      },
      "source": [
        "**Calculating Position Score**\n",
        "\n",
        "The position score for image i is simply the 2-norm of the position error (difference of estimated and ground truth position vectors), normalized with the ground truth distance of the satellite:\n",
        "\n",
        "score(i)position=∣∣r(i)gt−r(i)est∣∣2∣∣r(i)gt∣∣2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVgBqFY6wbRM",
        "colab_type": "code",
        "outputId": "8ee1f2dc-7570-4974-82e8-a33d2e876600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from numpy import linalg\n",
        "score_position=0\n",
        "for i in range(len(real_json)):\n",
        "   score_position+=(abs(linalg.norm([r_gt[i]],2)-linalg.norm([r_est[i]],2))/linalg.norm([r_gt[i]],2))\n",
        "\n",
        "#linalg.norm([r_gt[0]])\n",
        "score_position"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4496227202585083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZO0fome1Hil",
        "colab_type": "text"
      },
      "source": [
        "**Finally, the total score is the average of pose scores over all images of the test set:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7Yo9gyh1JD6",
        "colab_type": "code",
        "outputId": "a55539d7-abb4-4fce-901f-9ef0c3c54c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score=(score_orientation+score_position)/len(real_json)\n",
        "score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.2048093684503627"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}